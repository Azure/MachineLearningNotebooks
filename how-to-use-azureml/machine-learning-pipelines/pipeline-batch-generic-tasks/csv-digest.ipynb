{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing a large csv file in parallel as part of AzureML Pipelines\n",
    "\n",
    "This example will look at manipuating csv data using a Pipeline, however, this example can be extended to manipulating data to any scenario that you can achieve through a python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Data and Local Scenario\n",
    "\n",
    "We will be using an example csv that is structured like so: `Transaction_date,Product,Price,...` and summating the Price field to calculate the total revenue. However, any statisitcal or data transformation could be done.\n",
    "\n",
    "This data has already had the header removed, and split into 10 files as part of the preparation of doing the batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# take a look at one of the file contents\n",
    "file_path = os.path.join('sample-data', 'SalesJan2009.0.csv')\n",
    "with open(file_path, 'r') as csvfile:\n",
    "    for line in csvfile.read().splitlines()[:5]:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Locally, aggregating these totals would be easy to do. Just iterate over the rows of the file, select the transactionTotal field and summate the values. If we were to write this in code, it would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# ['SalesJan2009.0.csv', ... 'SalesJan2009.9.csv' ]\n",
    "files = [ 'SalesJan2009.{}.csv'.format(i) for i in range (0,10) ]\n",
    "\n",
    "for file in files:\n",
    "    total = 0\n",
    "    file_path = os.path.join('sample-data', file)\n",
    "    with open(file_path, 'r') as csvfile:\n",
    "        rows = csv.reader(csvfile)\n",
    "    \n",
    "        for fields in rows:\n",
    "            total += float(fields[2])\n",
    "\n",
    "    print('Total in {}: {:.2f}'.format(file, total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the Scenario to Pipelines\n",
    "\n",
    "For AzureML Pipeline to work we need to define three main things: our script to run, our inputs, and our outputs. For the script, we are almost done with the above logic, we just need to establish how our data will come into the pipeline and where the results of our transformation will be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Where our Data Will Live\n",
    "\n",
    "First we need to establish where our data will live. In the local scenario, it was just a file on our local machine, in the cloud it needs to be accessible by all machines that will be processing that information. First, we need to access our workspace, where we can arrange our data and/or AI resources.\n",
    "\n",
    "<strong>Note:</strong> If you're unfamiliar with creating a workspace, please visit [this helpful guide](https://docs.microsoft.com/en-us/azure/machine-learning/studio/create-workspace).\n",
    "We will need to access our cloud resouces to upload our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name,\n",
    "      'Azure region: ' + ws.location,\n",
    "      'Subscription id: ' + ws.subscription_id,\n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading our inputs to Datastore\n",
    "\n",
    "Now that we have our workspace loaded, we can upload our files to a datastore. This part can be skipped if it was already uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may be skipped if files were already uploaded\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "input_folder = datastore.upload(src_dir='sample-data', target_path='sample-data-on-cloud', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Already Uploaded File Inputs.\n",
    "\n",
    "If your data already exists within a datastore, you're able to  use this to reference that location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be ran if the above step was skipped. However, it won't hurt to run both.\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "input_folder = DataReference(datastore, path_on_datastore='sample-data-on-cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Output Folder\n",
    "\n",
    "This will be where the results of our batch job will be located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "# designate output location\n",
    "output_folder = PipelineData(name='output', datastore=datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our Python Script\n",
    "\n",
    "Let's edit our previous csv logic into a Pipelines python script. Pipelines expects each script to define a function called run, which it will be used as an entry point into executing your script. A second init function can be defined to ensure that the script's environment is constructed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_transactions.py\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "output_directory = 'results/'\n",
    "\n",
    "def init():\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "def run(files):\n",
    "    total = 0\n",
    "    for input_file in files:\n",
    "        # \"SalesJan2009.1.csv\" -> \"1\"\n",
    "        file_number = input_file.split('.')[1]\n",
    "        with open(input_file, 'r') as csvfile:\n",
    "            rows = csv.reader(csvfile)\n",
    "\n",
    "            for fields in rows:\n",
    "                total += float(fields[2])\n",
    "\n",
    "            result_file_path = os.path.join(output_directory, 'totals-{}.txt'.format(file_number))\n",
    "            with open(result_file_path) as result_file:\n",
    "                result_file.write('Total for {}: {}\\n'.format(day, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of opening local files, reading the contents, and writing the results to console; we are now reading from a list of files, writing the result to a file, and wrapped all of our logic in a function called run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Compute resources\n",
    "\n",
    "We will need something to run our Pipeline, here we will an aml compute cluster with only one vm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.webservice.batch import BatchServiceDeploymentConfiguration\n",
    "\n",
    "# AmlCompute\n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "try:\n",
    "    cpu_cluster = AmlCompute(ws, cpu_cluster_name)\n",
    "    print(\"found existing cluster.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new cluster\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_v2\",max_nodes=3)\n",
    "\n",
    "    # create the cluster\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, provisioning_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "# In a real-world scenario, you'll want to shape your process per node and nodes to fit your problem domain.\n",
    "deployment_config = BatchServiceDeploymentConfiguration(compute_target=cpu_cluster, \n",
    "                                                        node_count=3, process_count_per_node=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our Cloud Transform Step\n",
    "\n",
    "We need to communicate to AzureML how what we would like to compute. Although named BatchInferenceConfig for legacy reasons, this can accept any type of python script; for our purposes, this will be our csv totaling logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.model import BatchInferenceConfig\n",
    "from azureml.pipeline.steps.predictor_step import PredictorStep\n",
    "\n",
    "inference_config = BatchInferenceConfig(\n",
    "                    environment=Environment('batchinferencing'),\n",
    "                    entry_script='total_transactions.py',  # the user script to run against each input\n",
    "                    input_format='file',\n",
    "                    error_threshold=100,\n",
    "                    source_directory='./scripts',\n",
    "                    output_action='summary_only',\n",
    "                    description=None)\n",
    "\n",
    "# create a Predictor step for distributing style transfer step across multiple nodes in AmlCompute\n",
    "distributed_style_transfer_step = PredictorStep(\n",
    "    name='weekly-transactions',\n",
    "    inputs=[input_folder], # Input file share/blob container\n",
    "    output=output_folder, # Output file share/blob container\n",
    "    models=[],\n",
    "    arguments=['--logging_level', 'DEBUG', '--test', 'test'],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=deployment_config,\n",
    "    allow_reuse=False #[optional - default value True]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tieing Everything Together\n",
    "\n",
    "Now that we have a our data and pipeline configured, now all that is left is to construct the pipeline and execute the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[distributed_style_transfer_step])\n",
    "\n",
    "pipeline_run = Experiment(ws, 'file_summary_only').submit(pipeline, pipeline_params={'aml_node_count': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
