{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regular-yemen",
   "metadata": {},
   "source": [
    "# Extract, Transform, Load Audio Azure Machine Learning Pipeline\n",
    "\n",
    "The following jupytext notebook / script publishes and runs an Azure Machine Learning Pipeline to apply preprocessing functions to audio data contained in an Azure Blob Storage container.\n",
    "\n",
    "The following steps executed are\n",
    "- Extraction of audio from video files\n",
    "- Dynamic Range Compression to amplify main signal\n",
    "- Denoising to reduce background noise\n",
    "\n",
    "Note:\n",
    "\n",
    "This notebook is paired with the script `create_etl_pipeline.py` using [jupytext](https://github.com/mwouts/jupytext). Any updates to either this notebook or the script will result in changes for both files. This notebook is intended to serve as a previewable walkthrough of the script via Jupyter preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: skip-file\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython import get_ipython\n",
    "\n",
    "if get_ipython() is not None:\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "from azureml.core import Dataset, Environment, Experiment, Workspace\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from src.utils.aml import (\n",
    "    get_logger,\n",
    "    get_or_create_compute,\n",
    "    get_or_register_blob_datastore,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-taxation",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "log = get_logger(__name__)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "log.addHandler(stdout_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-canvas",
   "metadata": {},
   "source": [
    "## Setting Environment Variables\n",
    "\n",
    "Azure Machine Learning Compute Variables\n",
    "- `AML_COMPUTE_NAME` a compute name for the compute instance\n",
    "- `AML_COMPUTE_VM_PRIORITY` should be set to a VM size listed under \"Size\" per the names in the following doc https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs\n",
    "- `AML_COMPUTE_MIN_NODES` is the minimum number of nodes that will be allocated even when no pipelines are running\n",
    "- `AML_COMPUTE_MAX_NODES` is the maximum number of nodes to allocate even if there are more compute nodes requested than pipelines queued\n",
    "- `AML_COMPUTE_SCALE_DOWN` the amount of time in seconds for a compute node to stay idle before deallocating\n",
    "\n",
    "Azure Blob Storage Variables\n",
    "- `AML_BLOB_DATASTORE_NAME` the name to register on Azure Machine Learning for the associated Azure Blob Storage instance\n",
    "- `AML_BLOB_ACCOUNT_NAME` the account name for the Azure Blob Storage instance\n",
    "- `AML_BLOB_ACCOUNT_KEY` the account key for the Azure Blob Storage instance\n",
    "- `AML_BLOB_CONTAINER_NAME` the name of the container to read and write source video / audio files to processed audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Machine Learning (AML) Compute Variables\n",
    "AML_COMPUTE_NAME = os.getenv(\"AML_COMPUTE_NAME\")\n",
    "AML_COMPUTE_VM_SIZE = os.getenv(\"AML_COMPUTE_VM_SIZE\")\n",
    "AML_COMPUTE_VM_PRIORITY = os.getenv(\"AML_COMPUTE_VM_PRIORITY\")\n",
    "AML_COMPUTE_MIN_NODES = int(os.getenv(\"AML_COMPUTE_MIN_NODES\"))\n",
    "AML_COMPUTE_MAX_NODES = int(os.getenv(\"AML_COMPUTE_MAX_NODES\"))\n",
    "AML_COMPUTE_SCALE_DOWN = int(os.getenv(\"AML_COMPUTE_SCALE_DOWN\"))\n",
    "\n",
    "# Azure Blob Storage account to register to the AML Workspace\n",
    "AML_BLOB_DATASTORE_NAME = os.getenv(\"AML_BLOB_DATASTORE_NAME\")\n",
    "AML_BLOB_ACCOUNT_NAME = os.getenv(\"AML_BLOB_ACCOUNT_NAME\")\n",
    "AML_BLOB_ACCOUNT_KEY = os.getenv(\"AML_BLOB_ACCOUNT_KEY\")\n",
    "AML_BLOB_CONTAINER_NAME = os.getenv(\"AML_BLOB_CONTAINER_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-selling",
   "metadata": {},
   "source": [
    "## AML Workspace Config\n",
    "\n",
    "Reference the following AML doc https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment#workspace to obtain the `config.json` from your AML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore AML workspace from config.json file (can be downloaded through the portal)\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target = get_or_create_compute(\n",
    "    workspace=ws,\n",
    "    compute_name=AML_COMPUTE_NAME,\n",
    "    vm_size=AML_COMPUTE_VM_SIZE,\n",
    "    vm_priority=AML_COMPUTE_VM_PRIORITY,\n",
    "    min_nodes=AML_COMPUTE_MIN_NODES,\n",
    "    max_nodes=AML_COMPUTE_MAX_NODES,\n",
    "    scale_down=AML_COMPUTE_SCALE_DOWN,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-pepper",
   "metadata": {},
   "source": [
    "## Registering Azure Blob Storage\n",
    "\n",
    "Azure Machine Learning has a notion of [Datasets](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets) that are associated with a storage instance. The files will be read from a `DatasetConsumptionConfig` object generated during the `inputs` section of `ParallelRunStep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_datastore = get_or_register_blob_datastore(\n",
    "    workspace=ws,\n",
    "    datastore_name=AML_BLOB_DATASTORE_NAME,\n",
    "    storage_name=AML_BLOB_ACCOUNT_NAME,\n",
    "    storage_key=AML_BLOB_ACCOUNT_KEY,\n",
    "    container_name=AML_BLOB_CONTAINER_NAME,\n",
    ")\n",
    "\n",
    "root_dir = DataReference(\n",
    "    datastore=root_datastore, data_reference_name=\"source_files\", mode=\"mount\"\n",
    ")\n",
    "\n",
    "input_files = Dataset.File.from_files((root_datastore, \"source\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-collector",
   "metadata": {},
   "source": [
    "## Pipeline Parameters\n",
    "\n",
    "These parameters allow you to configure the pipeline run. Once the pipeline is published, these parameters can also be modified via the Azure Machine Learning Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PipelineParameter(name=\"output_dir\", default_value=\"outputs\")\n",
    "overwrite = PipelineParameter(name=\"overwrite\", default_value=True)\n",
    "\n",
    "# Sample Rate of 0 indicates that we will use the file's default\n",
    "sample_rate = PipelineParameter(name=\"sample_rate\", default_value=0)\n",
    "\n",
    "# Refer to documentation in steps/etl.py for the available options\n",
    "transform_order = PipelineParameter(\n",
    "    name=\"transform_order\", default_value=\"compress, denoise\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-sacrifice",
   "metadata": {},
   "source": [
    "## Configuring the Environment\n",
    "\n",
    "The environment is built off of the requirements specified in `requirements.in`. Note that the `lock` file equivalent is `requirements.txt` so dependencies may be incrementally updated if the docker image is re-built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment.from_pip_requirements(\"etl_audio\", \"requirements.in\")\n",
    "env.docker.enabled = True\n",
    "env.docker.base_image = None\n",
    "env.docker.base_dockerfile = \"./Dockerfile\"\n",
    "\n",
    "etl_config = ParallelRunConfig(\n",
    "    entry_script=\"mlops/steps/etl.py\",\n",
    "    mini_batch_size=\"1\",\n",
    "    error_threshold=0,\n",
    "    output_action=\"summary_only\",\n",
    "    compute_target=compute_target,\n",
    "    environment=env,\n",
    "    node_count=AML_COMPUTE_MAX_NODES,\n",
    "    run_invocation_timeout=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-tournament",
   "metadata": {},
   "source": [
    "## ParallelRunStep\n",
    "\n",
    "The `ParallelRunStep` receives as input a list of file paths to the source audio / video files on blob storage. After they are processed through the ETL step, they are written to the same container under `output_dir` with the same folder structure that the input files had.\n",
    "\n",
    "Note that the arguments receive a `PipelineParameter` as their input. The script `etl.py` uses `argparse` to receive these arguments and at runtime the `PipelineParameter` is converted to the standard type it represents such as `str`, `int`, `bool`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_step = ParallelRunStep(\n",
    "    name=\"etl\",\n",
    "    parallel_run_config=etl_config,\n",
    "    inputs=[input_files.as_named_input(\"input_files\").as_mount()],\n",
    "    side_inputs=[root_dir],\n",
    "    arguments=[\n",
    "        \"--base-dir\",\n",
    "        root_dir,\n",
    "        \"--output-dir\",\n",
    "        output_dir,\n",
    "        \"--overwrite\",\n",
    "        overwrite,\n",
    "        \"--sample-rate\",\n",
    "        sample_rate,\n",
    "        \"--transform-order\",\n",
    "        transform_order,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [etl_step]\n",
    "\n",
    "etl_pipeline = Pipeline(workspace=ws, steps=steps)\n",
    "etl_pipeline.validate()\n",
    "\n",
    "etl_pipeline.publish(\n",
    "    name=\"etl_pipeline\",\n",
    "    description=\"Extract, Transform, Load Pipeline for Audio Data\",\n",
    ")\n",
    "\n",
    "exp = Experiment(ws, \"etl_pipeline\").submit(etl_pipeline)\n",
    "exp.wait_for_completion()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
