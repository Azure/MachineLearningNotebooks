{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "Licensed under the MIT License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrate from Real Time Inference to Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show you the difference between using batch inference and real-time inference. Batch inference is optimized for high throughput, fire-and-forget predictions for a large collection of data. Real-time inference is more suitable for low-latency predicions. You may use one of them before, if so it's not difficult for you to use another. In the process of using them, they have the following major differences:\n",
    "- Batch inference needs to set up compute resources.\n",
    "- Batch inference needs Dataset for input data, a real-time inference can use user-uploaded data for input data.\n",
    "- Batch inference needs to build and run batch inference pipeline, real-time inference needs to deploy web service to MIR.\n",
    "> **Note** Please install azureml-pipeline-steps package and azureml-contrib-mir package before running this notebook.\n",
    "```\n",
    "pip install azureml-sdk\n",
    "pip install azureml-pipeline-steps\n",
    "pip install azureml-contrib-mir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common prerequisites\n",
    "Common prerequisites are work for both batch inference and real-time inference.\n",
    "### Connect to workspace\n",
    "Create a workspace object from the existing workspace, make sure you downloaded the config.json from your own workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering models with the workspace\n",
    "We will use the MNIST model for both batch inference and real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "# register the MNIST model \n",
    "model = Model.register(model_path = \"model/\",\n",
    "                       model_name = \"mnist\", # this is the name the model is registered as\n",
    "                       tags = {'pretrained': \"mnist\"},\n",
    "                       description = \"Mnist trained tensorflow model\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify then environment to your script\n",
    "Specify the conda dependencies for your script. This will allow us to install pip packages as well as configure the inference environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import CondaDependencies, DEFAULT_CPU_IMAGE\n",
    "\n",
    "# specify inference environment\n",
    "conda_deps = CondaDependencies.create(conda_packages=['numpy'], \n",
    "                                      pip_packages=[\"tensorflow==1.13.1\", \"pillow\", 'azureml-defaults'])\n",
    "myenv = Environment(name=\"my_environment\")\n",
    "myenv.python.conda_dependencies = conda_deps\n",
    "myenv.docker.enabled = True\n",
    "myenv.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference prequisites\n",
    "Following 3 steps only needed by batch inference.\n",
    "### Create or Attach existing compute target\n",
    "The code below creates the compute clusters for you if they don't already exist in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a FileDataset for input\n",
    "We have already created a public blob container `sampledata` on an account named `pipelinedata`, containing images from the MNIST dataset. We will create a FileDataset base on this blob container.\n",
    "Create a datastore with blob container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "account_name = \"pipelinedata\"\n",
    "datastore_name = \"mnist_datastore\"\n",
    "container_name = \"sampledata\"\n",
    "\n",
    "mnist_data = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=datastore_name, \n",
    "                      container_name= container_name, \n",
    "                      account_name=account_name,\n",
    "                      overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a FileDataset with the `mnist_data` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "mnist_ds_name = 'mnist_sample_data'\n",
    "\n",
    "path_on_datastore = mnist_data.path('mnist')\n",
    "input_mnist_ds = Dataset.File.from_files(path=path_on_datastore, validate=False)\n",
    "registered_mnist_ds = input_mnist_ds.register(ws, mnist_ds_name, create_new_version=True)\n",
    "named_mnist_ds = registered_mnist_ds.as_named_input(mnist_ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure output data\n",
    "Let's specify the default datastore for the outputs and construct our PipelineData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_data_store = ws.get_default_datastore()\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "\n",
    "output_dir = PipelineData(name=\"inferences\", \n",
    "                          datastore=def_data_store, \n",
    "                          output_path_on_compute=\"mnist/results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inference config\n",
    "### For batch inference, create a ParallelRunConfig and a pipeline step\n",
    "Create the ParallelRunConfig to wrap the inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=\"batch-script\",\n",
    "    entry_script=\"digit_identification.py\",\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=myenv,\n",
    "    compute_target=compute_target,\n",
    "    node_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline step using the script, environment configuration, and parameters. Specify the compute target you already attached to your workspace as the target of execution of the script. We will use ParallelRunStep to create the pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelrun_step = ParallelRunStep(\n",
    "    name=\"predict-digits-mnist\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[ named_mnist_ds ],\n",
    "    output=output_dir,\n",
    "    models=[ model ],\n",
    "    arguments=[ ],\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For real-time inference, create a InferenceConfig and deploy web service to MIR\n",
    "Create the InferenceConfig to warp the inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='real-time-script/score.py', environment=myenv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the web service configuration, deploy it to MIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.mir.webservice import MirWebservice\n",
    "\n",
    "mir_config = MirWebservice.deploy_configuration(cpu_cores=4, \n",
    "                                                memory_gb=4, \n",
    "                                                sku=\"Standard_NC6_Promo\", \n",
    "                                                num_replicas=1, \n",
    "                                                gpu_cores=1)\n",
    "mir_service_name ='mnist-mir-service'\n",
    "\n",
    "mir_service = Model.deploy(workspace = ws, \n",
    "                           name = mir_service_name,\n",
    "                           models = [model],\n",
    "                           inference_config = inference_config,\n",
    "                           deployment_config = mir_config)\n",
    "mir_service.wait_for_deployment(show_output = True)\n",
    "print(mir_service.state)\n",
    "print(mir_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the inference\n",
    "### Run the pipeline of batch inference\n",
    "At this point you can run the pipeline and examine the output it produced. The Experiment object is used to track the run of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
    "experiment = Experiment(ws, 'digit_identification')\n",
    "pipeline_run = experiment.submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run real-time inference by web service using run method\n",
    "We call the web service by passing data.\n",
    "Run() method retrieves API keys behind the scenes to make sure that call is authenticated. For real-time inference, the input data should not be too large, otherwise, you should use batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Used to test your webservice\n",
    "import os\n",
    "import urllib\n",
    "import gzip\n",
    "import numpy as np\n",
    "import struct\n",
    "import requests\n",
    "\n",
    "# load compressed MNIST gz files and return numpy arrays\n",
    "def load_data(filename, label=False):\n",
    "    with gzip.open(filename) as gz:\n",
    "        struct.unpack('I', gz.read(4))\n",
    "        n_items = struct.unpack('>I', gz.read(4))\n",
    "        if not label:\n",
    "            n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "            n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], n_rows * n_cols)\n",
    "        else:\n",
    "            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], 1)\n",
    "    return res\n",
    "\n",
    "# one-hot encode a 1-D array\n",
    "def one_hot_encode(array, num_of_classes):\n",
    "    return np.eye(num_of_classes)[array.reshape(-1)]\n",
    "\n",
    "# Download test data\n",
    "os.makedirs('./data/mnist', exist_ok=True)\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename='./data/mnist/test-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename='./data/mnist/test-labels.gz')\n",
    "\n",
    "# Load test data from model training\n",
    "X_test = load_data('./data/mnist/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"data\\\": [\" + str(list(X_test[random_index])) + \"]}\"\n",
    "\n",
    "token = mir_service.get_access_token().access_token\n",
    "headers = {'Content-Type': 'application/json',\n",
    "           'Authorization': ('Bearer ' + token)}\n",
    "resp = requests.post(mir_service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", mir_service.scoring_uri)\n",
    "print(\"label:\", y_test[random_index])\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup Compute resources\n",
    "\n",
    "For re-occurring jobs, it may be wise to keep compute the compute resources. However, since this is just a single-run job, we are free to release the allocated compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below and run if compute resources are no longer needed \n",
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
