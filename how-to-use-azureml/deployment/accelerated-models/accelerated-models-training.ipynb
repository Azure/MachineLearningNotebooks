{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/deployment/accelerated-models/accelerated-models-training.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training with the Azure Machine Learning Accelerated Models Service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook will introduce how to apply common machine learning techniques, like transfer learning, custom weights, and unquantized vs. quantized models, when working with our Azure Machine Learning Accelerated Models Service (Azure ML Accel Models).\n",
        "\n",
        "We will use Tensorflow for the preprocessing steps, ResNet50 for the featurizer, and the Keras API (built on Tensorflow backend) to build the classifier layers instead of the default ImageNet classifier used in Quickstart. Then we will train the model, evaluate it, and deploy it to run on an FPGA.\n",
        "\n",
        "#### Transfer Learning and Custom weights\n",
        "We will walk you through two ways to build and train a ResNet50 model on the Kaggle Cats and Dogs dataset: transfer learning only and then transfer learning with custom weights.\n",
        "\n",
        "In using transfer learning, our goal is to re-purpose the ResNet50 model already trained on the [ImageNet image dataset](http://www.image-net.org/) as a basis for our training of the Kaggle Cats and Dogs dataset. The ResNet50 featurizer will be imported as frozen, so only the Keras classifier will be trained.\n",
        "\n",
        "With the addition of custom weights, we will build the model so that the ResNet50 featurizer weights as not frozen. This will let us retrain starting with custom weights trained with ImageNet on ResNet50 and then use the Kaggle Cats and Dogs dataset to retrain and fine-tune the quantized version of the model.\n",
        "\n",
        "#### Unquantized vs. Quantized models\n",
        "The unquantized version of our models (ie. Resnet50, Resnet152, Densenet121, Vgg16, SsdVgg) uses native float precision (32-bit floats), which will be faster at training. We will use this for our first run through, then fine-tune the weights with the quantized version. The quantized version of our models (i.e. QuantizedResnet50, QuantizedResnet152, QuantizedDensenet121, QuantizedVgg16, QuantizedSsdVgg) will have the same node names as the unquantized version, but use quantized operations and will match the performance of the model when running on an FPGA.\n",
        "\n",
        "#### Contents\n",
        "1. [Setup Environment](#setup)\n",
        "* [Prepare Data](#prepare-data)\n",
        "* [Construct Model](#construct-model)\n",
        "    * Preprocessor\n",
        "    * Classifier\n",
        "    * Model construction\n",
        "* [Train Model](#train-model)\n",
        "* [Test Model](#test-model)\n",
        "* [Execution](#execution)\n",
        "    * [Transfer Learning](#transfer-learning)\n",
        "    * [Transfer Learning with Custom Weights](#custom-weights)\n",
        "* [Create Image](#create-image)\n",
        "* [Deploy Image](#deploy-image)\n",
        "* [Test the service](#test-service)\n",
        "* [Clean-up](#cleanup)\n",
        "* [Appendix](#appendix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"setup\"></a>\n",
        "## 1. Setup Environment\n",
        "#### 1.a. Please set up your environment as described in the [Quickstart](./accelerated-models-quickstart.ipynb), meaning:\n",
        "* Make sure your Workspace config.json exists and has the correct info\n",
        "* Install Tensorflow\n",
        "\n",
        "#### 1.b. Download dataset into ~/catsanddogs \n",
        "The dataset we will be using for training can be downloaded [here](https://www.microsoft.com/en-us/download/details.aspx?id=54765). Download the zip and extract to a directory named 'catsanddogs' under your user directory (\"~/catsanddogs\"). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.c. Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import sklearn\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.d. Create directories for later use\n",
        "After you train your model in float32, you'll write the weights to a place on disk. We also need a location to store the models that get downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_weights_dir = os.path.expanduser(\"~/custom-weights\")\n",
        "saved_model_dir = os.path.expanduser(\"~/models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"prepare-data\"></a>\n",
        "## 2. Prepare Data\n",
        "Load the files we are going to use for training and testing. By default this notebook uses only a very small subset of the Cats and Dogs dataset. That makes it run relatively quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import imghdr\n",
        "datadir = os.path.expanduser(\"~/catsanddogs\")\n",
        "\n",
        "cat_files = glob.glob(os.path.join(datadir, 'PetImages', 'Cat', '*.jpg'))\n",
        "dog_files = glob.glob(os.path.join(datadir, 'PetImages', 'Dog', '*.jpg'))\n",
        "\n",
        "# Limit the data set to make the notebook execute quickly.\n",
        "cat_files = cat_files[:64]\n",
        "dog_files = dog_files[:64]\n",
        "\n",
        "# The data set has a few images that are not jpeg. Remove them.\n",
        "cat_files = [f for f in cat_files if imghdr.what(f) == 'jpeg']\n",
        "dog_files = [f for f in dog_files if imghdr.what(f) == 'jpeg']\n",
        "\n",
        "if(not len(cat_files) or not len(dog_files)):\n",
        "    print(\"Please download the Kaggle Cats and Dogs dataset form https://www.microsoft.com/en-us/download/details.aspx?id=54765 and extract the zip to \" + datadir)    \n",
        "    raise ValueError(\"Data not found\")\n",
        "else:\n",
        "    print(cat_files[0])\n",
        "    print(dog_files[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct a numpy array as labels\n",
        "image_paths = cat_files + dog_files\n",
        "total_files = len(cat_files) + len(dog_files)\n",
        "labels = np.zeros(total_files)\n",
        "labels[len(cat_files):] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split images data as training data and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "onehot_labels = np.array([[0,1] if i else [1,0] for i in labels])\n",
        "img_train, img_test, label_train, label_test = train_test_split(image_paths, onehot_labels, random_state=42, shuffle=True)\n",
        "\n",
        "print(len(img_train), len(img_test), label_train.shape, label_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"construct-model\"></a>\n",
        "## 3. Construct Model\n",
        "We will define the functions to handle creating the preprocessor and the classifier first, and then run them together to actually construct the model with the Resnet50 featurizer in a single Tensorflow session in a separate cell.\n",
        "\n",
        "We use ResNet50 for the featurizer and build our own classifier using Keras layers. We train the featurizer and the classifier as one model. We will provide parameters to determine whether we are using the quantized version and whether we are using custom weights in training or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.a. Define image preprocessing step\n",
        "Same as in the Quickstart, before passing image dataset to the ResNet50 featurizer, we need to preprocess the input file to get it into the form expected by ResNet50. ResNet50 expects float tensors representing the images in BGR, channel last order. We've provided a default implementation of the preprocessing that you can use.\n",
        "\n",
        "**Note:** Expect to see TF deprecation warnings until we port our SDK over to use Tensorflow 2.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azureml.accel.models.utils as utils\n",
        "\n",
        "def preprocess_images(scaling_factor=1.0):\n",
        "    # Convert images to 3D tensors [width,height,channel] - channels are in BGR order.\n",
        "    in_images = tf.placeholder(tf.string)\n",
        "    image_tensors = utils.preprocess_array(in_images, 'RGB', scaling_factor)\n",
        "    return in_images, image_tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.b. Define classifier\n",
        "We use Keras layer APIs to construct the classifier. Because we're using the tensorflow backend, we can train this classifier in one session with our Resnet50 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_classifier(in_tensor, seed=None):\n",
        "    from keras.layers import Dropout, Dense, Flatten\n",
        "    from keras.initializers import glorot_uniform\n",
        "    K.set_session(tf.get_default_session())\n",
        "\n",
        "    FC_SIZE = 1024\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "    x = Dropout(0.2, input_shape=(1, 1, int(in_tensor.shape[3]),), seed=seed)(in_tensor)\n",
        "    x = Dense(FC_SIZE, activation='relu', input_dim=(1, 1, int(in_tensor.shape[3]),),\n",
        "              kernel_initializer=glorot_uniform(seed=seed), bias_initializer='zeros')(x)\n",
        "    x = Flatten()(x)\n",
        "    preds = Dense(NUM_CLASSES, activation='softmax', input_dim=FC_SIZE, name='classifier_output',\n",
        "                  kernel_initializer=glorot_uniform(seed=seed), bias_initializer='zeros')(x)\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.c. Define model construction\n",
        "Now that the preprocessor and classifier for the model are defined, we can define how we want to construct the model. \n",
        "\n",
        "Constructing the model has these steps: \n",
        "1. Get preprocessing steps\n",
        "* Get featurizer using the Azure ML Accel Models SDK:\n",
        "    * import the graph definition\n",
        "    * restore the weights of the model into a Tensorflow session\n",
        "* Get classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_model(quantized, starting_weights_directory = None):\n",
        "    from azureml.accel.models import Resnet50, QuantizedResnet50\n",
        "    \n",
        "    # Convert images to 3D tensors [width,height,channel]\n",
        "    in_images, image_tensors = preprocess_images(1.0)\n",
        "\n",
        "    # Construct featurizer using quantized or unquantized ResNet50 model\n",
        "    if not quantized:\n",
        "        featurizer = Resnet50(saved_model_dir)\n",
        "    else:\n",
        "        featurizer = QuantizedResnet50(saved_model_dir, custom_weights_directory = starting_weights_directory)\n",
        "\n",
        "    features = featurizer.import_graph_def(input_tensor=image_tensors)\n",
        "    \n",
        "    # Construct classifier\n",
        "    preds = construct_classifier(features)\n",
        "    \n",
        "    # Initialize weights\n",
        "    sess = tf.get_default_session()\n",
        "    tf.global_variables_initializer().run()\n",
        "\n",
        "    featurizer.restore_weights(sess)\n",
        "\n",
        "    return in_images, image_tensors, features, preds, featurizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"train-model\"></a>\n",
        "## 4. Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_files(files):\n",
        "    \"\"\" Read files to array\"\"\"\n",
        "    contents = []\n",
        "    for path in files:\n",
        "        with open(path, 'rb') as f:\n",
        "            contents.append(f.read())\n",
        "    return contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(preds, in_images, img_train, label_train, is_retrain = False, train_epoch = 10, learning_rate=None):\n",
        "    \"\"\" training model \"\"\"\n",
        "    from keras.objectives import binary_crossentropy\n",
        "    from tqdm import tqdm\n",
        "    \n",
        "    learning_rate = learning_rate if learning_rate else 0.001 if is_retrain else 0.01\n",
        "        \n",
        "    # Specify the loss function\n",
        "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))   \n",
        "    cross_entropy = tf.reduce_mean(binary_crossentropy(in_labels, preds))\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
        "\n",
        "    def chunks(a, b, n):\n",
        "        \"\"\"Yield successive n-sized chunks from a and b.\"\"\"\n",
        "        if (len(a) != len(b)):\n",
        "            print(\"a and b are not equal in chunks(a,b,n)\")\n",
        "            raise ValueError(\"Parameter error\")\n",
        "\n",
        "        for i in range(0, len(a), n):\n",
        "            yield a[i:i + n], b[i:i + n]\n",
        "\n",
        "    chunk_size = 16\n",
        "    chunk_num = len(label_train) / chunk_size\n",
        "\n",
        "    sess = tf.get_default_session()\n",
        "    for epoch in range(train_epoch):\n",
        "        avg_loss = 0\n",
        "        for img_chunk, label_chunk in tqdm(chunks(img_train, label_train, chunk_size)):\n",
        "            contents = read_files(img_chunk)\n",
        "            _, loss = sess.run([optimizer, cross_entropy],\n",
        "                                feed_dict={in_images: contents,\n",
        "                                           in_labels: label_chunk,\n",
        "                                           K.learning_phase(): 1})\n",
        "            avg_loss += loss / chunk_num\n",
        "        print(\"Epoch:\", (epoch + 1), \"loss = \", \"{:.3f}\".format(avg_loss))\n",
        "            \n",
        "        # Reach desired performance\n",
        "        if (avg_loss < 0.001):\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"test-model\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"test-model\"></a>\n",
        "## 5. Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(preds, in_images, img_test, label_test):\n",
        "    \"\"\"Test the model\"\"\"\n",
        "    from keras.metrics import categorical_accuracy\n",
        "\n",
        "    in_labels = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "    accuracy = tf.reduce_mean(categorical_accuracy(in_labels, preds))\n",
        "    contents = read_files(img_test)\n",
        "\n",
        "    accuracy = accuracy.eval(feed_dict={in_images: contents,\n",
        "                                        in_labels: label_test,\n",
        "                                        K.learning_phase(): 0})\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"execution\"></a>\n",
        "## 6. Execute steps\n",
        "You can run through the Transfer Learning section, then skip to Create AccelContainerImage. By default, because the custom weights section takes much longer for training twice, it is not saved as executable cells. You can copy the code or change cell type to 'Code'.\n",
        "\n",
        "<a id=\"transfer-learning\"></a>\n",
        "### 6.a. Training using Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Launch the training\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session(graph=tf.get_default_graph())\n",
        "\n",
        "with sess.as_default():\n",
        "    in_images, image_tensors, features, preds, featurizer = construct_model(quantized=True)\n",
        "    train_model(preds, in_images, img_train, label_train, is_retrain=False, train_epoch=10, learning_rate=0.01)    \n",
        "    accuracy = test_model(preds, in_images, img_test, label_test)  \n",
        "    print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'resnet50-catsanddogs-tl'\n",
        "model_save_path = os.path.join(saved_model_dir, model_name)\n",
        "\n",
        "tf.saved_model.simple_save(sess, model_save_path,\n",
        "                               inputs={'images': in_images},\n",
        "                               outputs={'output_alias': preds})\n",
        "\n",
        "input_tensors = in_images.name\n",
        "output_tensors = preds.name\n",
        "\n",
        "print(input_tensors)\n",
        "print(output_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"custom-weights\"></a>\n",
        "### 6.b. Traning using Custom Weights\n",
        "\n",
        "Because the quantized graph defintion and the float32 graph defintion share the same node names in the graph definitions, we can initally train the weights in float32, and then reload them with the quantized operations (which take longer) to fine-tune the model.\n",
        "\n",
        "First we train the model with custom weights but without quantization. Training is done with native float precision (32-bit floats). We load the training data set and batch the training with 10 epochs. When the performance reaches desired level or starts decredation, we stop the training iteration and save the weights as tensorflow checkpoint files. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Launch the training\n",
        "```\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session(graph=tf.get_default_graph())\n",
        "\n",
        "with sess.as_default():\n",
        "    in_images, image_tensors, features, preds, featurizer = construct_model(quantized=False)\n",
        "    train_model(preds, in_images, img_train, label_train, is_retrain=False, train_epoch=10)    \n",
        "    accuracy = test_model(preds, in_images, img_test, label_test)  \n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    featurizer.save_weights(custom_weights_dir + \"/rn50\", tf.get_default_session())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test Model\n",
        "After training, we evaluate the trained model's accuracy on test dataset with quantization. So that we know the model's performance if it is deployed on the FPGA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "tf.reset_default_graph()\n",
        "sess = tf.Session(graph=tf.get_default_graph())\n",
        "\n",
        "with sess.as_default():\n",
        "    print(\"Testing trained model with quantization\")\n",
        "    in_images, image_tensors, features, preds, quantized_featurizer = construct_model(quantized=True, starting_weights_directory=custom_weights_dir)\n",
        "    accuracy = test_model(preds, in_images, img_test, label_test)      \n",
        "    print(\"Accuracy:\", accuracy)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine-Tune Model\n",
        "Sometimes, the model's accuracy can drop significantly after quantization. In those cases, we need to retrain the model enabled with quantization to get better model accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "if (accuracy < 0.93):\n",
        "    with sess.as_default():\n",
        "        print(\"Fine-tuning model with quantization\")\n",
        "        train_model(preds, in_images, img_train, label_train, is_retrain=True, train_epoch=10)\n",
        "        accuracy = test_model(preds, in_images, img_test, label_test)        \n",
        "        print(\"Accuracy:\", accuracy)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "model_name = 'resnet50-catsanddogs-cw'\n",
        "model_save_path = os.path.join(saved_model_dir, model_name)\n",
        "\n",
        "tf.saved_model.simple_save(sess, model_save_path,\n",
        "                               inputs={'images': in_images},\n",
        "                               outputs={'output_alias': preds})\n",
        "\n",
        "input_tensors = in_images.name\n",
        "output_tensors = preds.name\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"create-image\"></a>\n",
        "## 7. Create AccelContainerImage\n",
        "\n",
        "Below we will execute all the same steps as in the [Quickstart](./accelerated-models-quickstart.ipynb#create-image) to package the model we have saved locally into an accelerated Docker image saved in our workspace. To complete all the steps, it may take a few minutes. For more details on each step, check out the [Quickstart section on model registration](./accelerated-models-quickstart.ipynb#register-model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.image import Image\n",
        "from azureml.accel import AccelOnnxConverter\n",
        "from azureml.accel import AccelContainerImage\n",
        "\n",
        "# Retrieve workspace\n",
        "ws = Workspace.from_config()\n",
        "print(\"Successfully retrieved workspace:\", ws.name, ws.resource_group, ws.location, ws.subscription_id, '\\n')\n",
        "\n",
        "# Register model\n",
        "registered_model = Model.register(workspace = ws,\n",
        "                                  model_path = model_save_path,\n",
        "                                  model_name = model_name)\n",
        "print(\"Successfully registered: \", registered_model.name, registered_model.description, registered_model.version, '\\n', sep = '\\t')\n",
        "\n",
        "# Convert model\n",
        "convert_request = AccelOnnxConverter.convert_tf_model(ws, registered_model, input_tensors, output_tensors)\n",
        "if convert_request.wait_for_completion(show_output = False):\n",
        "    # If the above call succeeded, get the converted model\n",
        "    converted_model = convert_request.result\n",
        "    print(\"\\nSuccessfully converted: \", converted_model.name, converted_model.url, converted_model.version, \n",
        "          converted_model.id, converted_model.created_time, '\\n')\n",
        "else:\n",
        "    print(\"Model conversion failed. Showing output.\")\n",
        "    convert_request.wait_for_completion(show_output = True)\n",
        "\n",
        "# Package into AccelContainerImage\n",
        "image_config = AccelContainerImage.image_configuration()\n",
        "# Image name must be lowercase\n",
        "image_name = \"{}-image\".format(model_name)\n",
        "image = Image.create(name = image_name,\n",
        "                     models = [converted_model],\n",
        "                     image_config = image_config, \n",
        "                     workspace = ws)\n",
        "image.wait_for_creation()\n",
        "print(\"Created AccelContainerImage: {} {} {}\\n\".format(image.name, image.creation_state, image.image_location))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"deploy-image\"></a>\n",
        "## 8. Deploy image\n",
        "Once you have an Azure ML Accelerated Image in your Workspace, you can deploy it to two destinations, to a Databox Edge machine or to an AKS cluster. \n",
        "\n",
        "### 8.a. Deploy to Databox Edge Machine using IoT Hub\n",
        "See the sample [here](https://github.com/Azure-Samples/aml-real-time-ai/) for using the Azure IoT CLI extension for deploying your Docker image to your Databox Edge Machine.\n",
        "\n",
        "### 8.b. Deploy to AKS Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create AKS ComputeTarget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.compute import AksCompute, ComputeTarget\n",
        "\n",
        "# Uses the specific FPGA enabled VM (sku: Standard_PB6s)\n",
        "# Standard_PB6s are available in: eastus, westus2, westeurope, southeastasia\n",
        "prov_config = AksCompute.provisioning_configuration(vm_size = \"Standard_PB6s\",\n",
        "                                                    agent_count = 1,\n",
        "                                                    location = \"eastus\")\n",
        "\n",
        "aks_name = 'aks-pb6-tl'\n",
        "# Create the cluster\n",
        "aks_target = ComputeTarget.create(workspace = ws, \n",
        "                                  name = aks_name, \n",
        "                                  provisioning_configuration = prov_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provisioning an AKS cluster might take awhile (15 or so minutes), and we want to wait until it's successfully provisioned before we can deploy a service to it. If you interrupt this cell, provisioning of the cluster will continue. You can re-run it or check the status in your Workspace under Compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "aks_target.wait_for_completion(show_output = True)\n",
        "print(aks_target.provisioning_state)\n",
        "print(aks_target.provisioning_errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Deploy AccelContainerImage to AKS ComputeTarget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from azureml.core.webservice import Webservice, AksWebservice\n",
        "\n",
        "# Set the web service configuration (for creating a test service, we don't want autoscale enabled)\n",
        "# Authentication is enabled by default, but for testing we specify False\n",
        "aks_config = AksWebservice.deploy_configuration(autoscale_enabled=False,\n",
        "                                                num_replicas=1,\n",
        "                                                auth_enabled = False)\n",
        "\n",
        "aks_service_name ='my-aks-service'\n",
        "\n",
        "aks_service = Webservice.deploy_from_image(workspace = ws,\n",
        "                                           name = aks_service_name,\n",
        "                                           image = image,\n",
        "                                           deployment_config = aks_config,\n",
        "                                           deployment_target = aks_target)\n",
        "aks_service.wait_for_deployment(show_output = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"test-service\"></a>\n",
        "## 9. Test the service\n",
        "\n",
        "<a id=\"create-client\"></a>\n",
        "### 9.a. Create Client\n",
        "The image supports gRPC and the TensorFlow Serving \"predict\" API. We will create a PredictionClient from the Webservice object that can call into the docker image to get predictions. If you do not have the Webservice object, you can also create [PredictionClient](https://docs.microsoft.com/en-us/python/api/azureml-accel-models/azureml.accel.predictionclient?view=azure-ml-py) directly.\n",
        "\n",
        "**Note:** If you chose to use auth_enabled=True when creating your AksWebservice.deploy_configuration(), see documentation [here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice(class)?view=azure-ml-py#get-keys--) on how to retrieve your keys and use either key as an argument to PredictionClient(...,access_token=key).\n",
        "**WARNING:** If you are running on Azure Notebooks free compute, you will not be able to make outgoing calls to your service. Try locating your client on a different machine to consume it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the grpc client in AzureML Accelerated Models SDK\n",
        "from azureml.accel import client_from_service\n",
        "\n",
        "# Initialize AzureML Accelerated Models client\n",
        "client = client_from_service(aks_service)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"serve-model\"></a>\n",
        "### 9.b. Serve the model\n",
        "Let's see how our service does on a few images. It may get a few wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify an image to classify\n",
        "print('CATS')\n",
        "for image_file in cat_files[:8]:\n",
        "    results = client.score_file(path=image_file, \n",
        "                                 input_name=input_tensors, \n",
        "                                 outputs=output_tensors)\n",
        "    result = 'CORRECT ' if results[0] > results[1] else 'WRONG '\n",
        "    print(result + str(results))\n",
        "print('DOGS')\n",
        "for image_file in dog_files[:8]:\n",
        "    results = client.score_file(path=image_file, \n",
        "                                 input_name=input_tensors, \n",
        "                                 outputs=output_tensors)\n",
        "    result = 'CORRECT ' if results[1] > results[0] else 'WRONG '\n",
        "    print(result + str(results))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"cleanup\"></a>\n",
        "## 10. Cleanup\n",
        "It's important to clean up your resources, so that you won't incur unnecessary costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aks_service.delete()\n",
        "aks_target.delete()\n",
        "image.delete()\n",
        "registered_model.delete()\n",
        "converted_model.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"appendix\"></a>\n",
        "## 11. Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "License for plot_confusion_matrix:\n",
        "\n",
        "New BSD License\n",
        "\n",
        "Copyright (c) 2007-2018 The scikit-learn developers.\n",
        "All rights reserved.\n",
        "\n",
        "\n",
        "Redistribution and use in source and binary forms, with or without\n",
        "modification, are permitted provided that the following conditions are met:\n",
        "\n",
        "  a. Redistributions of source code must retain the above copyright notice,\n",
        "     this list of conditions and the following disclaimer.\n",
        "  b. Redistributions in binary form must reproduce the above copyright\n",
        "     notice, this list of conditions and the following disclaimer in the\n",
        "     documentation and/or other materials provided with the distribution.\n",
        "  c. Neither the name of the Scikit-learn Developers  nor the names of\n",
        "     its contributors may be used to endorse or promote products\n",
        "     derived from this software without specific prior written\n",
        "     permission. \n",
        "\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
        "AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        "IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
        "ARE DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR\n",
        "ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
        "DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
        "SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
        "CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n",
        "LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n",
        "OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n",
        "DAMAGE.\n"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "coverste"
      },
      {
        "name": "paledger"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}