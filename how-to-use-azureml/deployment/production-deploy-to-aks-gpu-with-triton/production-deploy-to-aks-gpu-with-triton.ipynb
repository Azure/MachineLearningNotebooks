{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying a web service hosted on NVIDIA Triton to Azure Kubernetes Service (AKS)\n",
        "This notebook shows the steps for deploying a service with [NVIDIA Triton Inferencing Server](https://developer.nvidia.com/nvidia-triton-inference-server): registering a model, creating an image, provisioning a cluster (one time action), and deploying a service to it. \n",
        "We then test and delete the service, image and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "print(azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get workspace\n",
        "Load existing workspace from the config file info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from azureml.core.workspace import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download the model\n",
        "\n",
        "Prior to registering the model, you should have a model in one of the [supported formats](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#framework-model-definition) by Triton. This cell will download a [pretrained ONNX densenet](https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx) model.\n",
        "\n",
        "** Note: ** If you have a previously-registered model, the file name needs to follow the [naming convention](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html?highlight=model%20onnx#framework-model-definition) or be specified using model configuration file below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from io import BytesIO\n",
        "\n",
        "model_url = 'https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx'\n",
        "\n",
        "version = '1'\n",
        "model_dir = os.path.join('models', 'triton', 'densenet_onnx', version)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "target_file = os.path.join(model_dir, 'model.onnx')\n",
        "\n",
        "if not os.path.exists(target_file):\n",
        "    response = requests.get(model_url)\n",
        "    open(target_file, 'wb').write(response.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Add Model Configuration file\n",
        "\n",
        "Each Triton model needs a [Model Configuration](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_configuration.html) that provides required and optional information about the model. We've provided one in this directory already.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Register the model\n",
        "Register an existing trained model, add description and tags.\n",
        "\n",
        "** Note: ** Under `model_path` there must be a sub-directory named `triton`, which has the structure of a Triton [Model Repository](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#repository-layout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = Model.register(model_path=\"models\", # This points to the local directory to upload.\n",
        "                       model_name=\"densenet_onnx\", # This is the name the model is registered as.\n",
        "                       tags={'area': \"Image classification\", 'type': \"classification\"},\n",
        "                       description=\"Image classification trained on Imagenet Dataset\",\n",
        "                       workspace=ws)\n",
        "\n",
        "print(model.name, model.description, model.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Provision the AKS Cluster\n",
        "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AksCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your GPU cluster\n",
        "gpu_cluster_name = \"aks-gpu-cluster\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
        "    print(\"Found existing gpu cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Creating new gpu-cluster\")\n",
        "    \n",
        "    # Specify the configuration for the new cluster\n",
        "    compute_config = AksCompute.provisioning_configuration(\n",
        "        cluster_purpose=AksCompute.ClusterPurpose.DEV_TEST,\n",
        "        agent_count=1,\n",
        "        vm_size=\"Standard_NC6\",\n",
        "        location=\"westus2\")\n",
        "    \n",
        "    # Create the cluster with the specified name and configuration\n",
        "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
        "\n",
        "    # Wait for the cluster to complete, show the output log\n",
        "    gpu_cluster.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy the model as a web service to AKS\n",
        "\n",
        "First create a scoring script. You can see the one we created for you in the `scripts` directory.\n",
        "\n",
        "** Note: ** Triton server listens to a fixed local port. You may choose to use the Triton Python [client library](https://docs.nvidia.com/deeplearning/triton-inference-server/master-user-guide/docs/client_library.html) to talk to it, while keeping the flexibility of pre-/post- processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "** Note: ** If you use the Triton Python client, download the Wheels and include them in the `Environment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "triton_client_url = 'https://github.com/NVIDIA/triton-inference-server/releases/download/v2.1.0/v2.1.0_ubuntu1804.clients.tar.gz'\n",
        "\n",
        "client_filename = 'tritonclientutils-2.1.0-py3-none-any.whl'\n",
        "clientutils_filename = 'tritonhttpclient-2.1.0-py3-none-any.whl'\n",
        "target_folder = 'python_client'\n",
        "\n",
        "if not os.path.exists(target_folder):\n",
        "    response = requests.get(triton_client_url)\n",
        "    archive = tarfile.open(fileobj=BytesIO(response.content))\n",
        "    with tempfile.TemporaryDirectory() as temp_folder:\n",
        "        archive.extractall(temp_folder)\n",
        "        os.mkdir(target_folder)\n",
        "        shutil.copy(os.path.join(temp_folder, 'python', client_filename), target_folder)\n",
        "        shutil.copy(os.path.join(temp_folder, 'python', clientutils_filename), target_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create the deployment configuration objects and deploy the model as a webservice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Set the web service configuration (using default here)\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AksWebservice\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.environment import Environment\n",
        "\n",
        "env = Environment(\"deploytocloudenv\")\n",
        "conda_dep = CondaDependencies(\"env.yml\")\n",
        "client_whl_url = Environment.add_private_pip_wheel(workspace=ws, file_path = os.path.join(target_folder, client_filename), exist_ok=True)\n",
        "clientutils_whl_url = Environment.add_private_pip_wheel(workspace=ws, file_path = os.path.join(target_folder, clientutils_filename), exist_ok=True)\n",
        "conda_dep.add_pip_package(client_whl_url)\n",
        "conda_dep.add_pip_package(clientutils_whl_url)\n",
        "env.python.conda_dependencies = conda_dep\n",
        "\n",
        "# TODO: replace with MCR image name\n",
        "#env.docker.base_image = DEFAULT_GPU_IMAGE\n",
        "env.docker.base_image = \"delliwscentrec595b96.azurecr.io/azureml-tritonserver:20.07-py3\"\n",
        "env.docker.base_image_registry.address = \"delliwscentrec595b96.azurecr.io\"\n",
        "env.docker.base_image_registry.username = \"delliwscentrec595b96\"\n",
        "env.docker.base_image_registry.password = \"eOlKQlx+tFa+J3Ml5a86fOHf8i=hi/ec\"\n",
        "\n",
        "# Optionally specify a worker count to leverage the capability of concurrency and server-side batching from Triton\n",
        "# env.environment_variables = {\"WORKER_COUNT\":\"128\"}\n",
        "\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", source_directory=\"scripts\", environment=env)\n",
        "aks_config = AksWebservice.deploy_configuration(cpu_cores=1, memory_gb=4, gpu_cores=1)\n",
        "\n",
        "# # Enable token auth and disable (key) auth on the webservice\n",
        "# aks_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 4, gpu_cores = 1, token_auth_enabled=True, auth_enabled=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "aks_service_name ='gpu-densenet-onnx'\n",
        "\n",
        "aks_service = Model.deploy(workspace=ws,\n",
        "                           name=aks_service_name,\n",
        "                           models=[model],\n",
        "                           inference_config=inference_config,\n",
        "                           deployment_config=aks_config,\n",
        "                           deployment_target=gpu_cluster)\n",
        "\n",
        "aks_service.wait_for_deployment(show_output = True)\n",
        "print(aks_service.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test the web service\n",
        "We test the web sevice by passing the test images content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import requests\n",
        "\n",
        "# if (key) auth is enabled, fetch keys and include in the request\n",
        "key1, key2 = aks_service.get_keys()\n",
        "\n",
        "headers = {'Content-Type':'application/octet-stream', 'Authorization': 'Bearer ' + key1}\n",
        "\n",
        "# # if token auth is enabled, fetch token and include in the request\n",
        "# access_token, fetch_after = aks_service.get_token()\n",
        "# headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + access_token}\n",
        "\n",
        "test_sample = open('car.jpg', 'rb').read()\n",
        "resp = requests.post(aks_service.scoring_uri, test_sample, headers=headers)\n",
        "print(resp.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean up\n",
        "Delete the service, image, model and compute target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "aks_service.delete()\n",
        "model.delete()\n",
        "gpu_cluster.delete()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "aks_service.wait_for_deployment(True)"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "vaidyas"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3.7.7 64-bit",
      "language": "python",
      "name": "python_defaultSpec_1598590942828"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}