{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/production-deploy-to-aks-gpu/production-deploy-to-aks-gpu.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploying a web service hosted on NVIDIA Triton to Azure Kubernetes Service (AKS)\n",
        "This notebook shows the steps for deploying a service with [NVIDIA Triton Inferencing Server](https://developer.nvidia.com/nvidia-triton-inference-server): registering a model, creating an image, provisioning a cluster (one time action), and deploying a service to it. \n",
        "We then test and delete the service, image and model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "print(azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get workspace\n",
        "Load existing workspace from the config file info."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.workspace import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download the model\n",
        "\n",
        "Prior to registering the model, you should have a model in one of the [supported formats](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#framework-model-definition) by Triton. This cell will download a [pretrained ONNX densenet](https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx) model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import shutil\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from io import BytesIO\n",
        "\n",
        "model_url = 'https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx'\n",
        "\n",
        "target_file = os.path.join('models', 'triton', 'densenet_onnx', '1', 'model.onnx')\n",
        "\n",
        "if not os.path.exists(target_file):\n",
        "    response = requests.get(model_url)\n",
        "    open(target_file, 'wb').write(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Register the model\n",
        "Register an existing trained model, add description and tags.\n",
        "\n",
        "** Note: ** Under `model_path` there must be a sub-directory named `triton`, which has the structure of a Triton [Model Repository](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html#repository-layout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model = Model.register(model_path=\"models\", # This points to the local directory to upload.\n",
        "                       model_name=\"densenet_onnx\", # This is the name the model is registered as.\n",
        "                       tags={'area': \"Image classification\", 'type': \"classification\"},\n",
        "                       description=\"Image classification trained on Imagenet Dataset\",\n",
        "                       workspace=ws)\n",
        "\n",
        "print(model.name, model.description, model.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Provision the AKS Cluster\n",
        "This is a one time setup. You can reuse this cluster for multiple deployments after it has been created. If you delete the cluster or the resource group that contains it, then you would have to recreate it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AksCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your GPU cluster\n",
        "gpu_cluster_name = \"aks-gpu-cluster\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
        "    print(\"Found existing gpu cluster\")\n",
        "except ComputeTargetException:\n",
        "    print(\"Creating new gpu-cluster\")\n",
        "    \n",
        "    # Specify the configuration for the new cluster\n",
        "    compute_config = AksCompute.provisioning_configuration(cluster_purpose=AksCompute.ClusterPurpose.DEV_TEST,\n",
        "                                                           agent_count=1)\n",
        "                                                           #vm_size=\"Standard_NV6\")\n",
        "    # Create the cluster with the specified name and configuration\n",
        "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
        "\n",
        "    # Wait for the cluster to complete, show the output log\n",
        "    gpu_cluster.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deploy the model as a web service to AKS\n",
        "\n",
        "First create a scoring script\n",
        "\n",
        "** Note: ** Triton server listens to a fixed local port. You may choose to use the Triton python client to talk to it, while keeping the flexibility of pre-/post- processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%writefile score.py\n",
        "import argparse\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import sys\n",
        "from functools import partial\n",
        "import os\n",
        "import io\n",
        "\n",
        "import tritonhttpclient\n",
        "from tritonclientutils import triton_to_np_dtype\n",
        "from tritonclientutils import InferenceServerException\n",
        "\n",
        "from azureml.contrib.services.aml_request import AMLRequest, rawhttp\n",
        "from azureml.contrib.services.aml_response import AMLResponse\n",
        "\n",
        "def preprocess(img, scaling):\n",
        "    \"\"\"\n",
        "    Pre-process an image to meet the size, type and format\n",
        "    requirements specified by the parameters.\n",
        "    \"\"\"\n",
        "    # np.set_printoptions(threshold='nan')\n",
        "    c = 3\n",
        "    h = 224\n",
        "    w = 224\n",
        "    format = \"FORMAT_NCHW\"\n",
        "\n",
        "    \n",
        "    if c == 1:\n",
        "        sample_img = img.convert('L')\n",
        "    else:\n",
        "        sample_img = img.convert('RGB')\n",
        "\n",
        "    resized_img = sample_img.resize((w, h), Image.BILINEAR)\n",
        "    resized = np.array(resized_img)\n",
        "    if resized.ndim == 2:\n",
        "        resized = resized[:, :, np.newaxis]\n",
        "\n",
        "    npdtype = triton_to_np_dtype(dtype)\n",
        "    typed = resized.astype(npdtype)\n",
        "\n",
        "    if scaling == 'INCEPTION':\n",
        "        scaled = (typed / 128) - 1\n",
        "    elif scaling == 'VGG':\n",
        "        if c == 1:\n",
        "            scaled = typed - np.asarray((128,), dtype=npdtype)\n",
        "        else:\n",
        "            scaled = typed - np.asarray((123, 117, 104), dtype=npdtype)\n",
        "    else:\n",
        "        scaled = typed\n",
        "\n",
        "    # Swap to CHW if necessary\n",
        "    if format == \"FORMAT_NCHW\":\n",
        "        ordered = np.transpose(scaled, (2, 0, 1))\n",
        "    else:\n",
        "        ordered = scaled\n",
        "\n",
        "    # Channels are in RGB order. Currently model configuration data\n",
        "    # doesn't provide any information as to other channel orderings\n",
        "    # (like BGR) so we just assume RGB.\n",
        "    return ordered\n",
        "\n",
        "\n",
        "def postprocess(results, output_name, batch_size, batching):\n",
        "    \"\"\"\n",
        "    Post-process results to show classifications.\n",
        "    \"\"\"\n",
        "\n",
        "    output_array = results.as_numpy(output_name)\n",
        "    if len(output_array) != batch_size:\n",
        "        raise Exception(\"expected {} results, got {}\".format(\n",
        "            batch_size, len(output_array)))\n",
        "\n",
        "    # Include special handling for non-batching models\n",
        "    output = \"\"\n",
        "    for results in output_array:\n",
        "        if not batching:\n",
        "            results = [results]\n",
        "        for result in results:\n",
        "            if output_array.dtype.type == np.bytes_:\n",
        "                cls = \"\".join(chr(x) for x in result).split(':')\n",
        "            else:\n",
        "                cls = result.split(':')\n",
        "            output += \"    {} ({}) = {}\".format(cls[0], cls[1], cls[2])\n",
        "\n",
        "    return output\n",
        "\n",
        "trition_client = None\n",
        "_url = \"localhost:8000\"\n",
        "_model = \"densenet_onnx\"\n",
        "_scaling = \"INCEPTION\"\n",
        "\n",
        "def init():\n",
        "    global triton_client, max_batch_size, input_name, output_name, dtype\n",
        "    \n",
        "    print(\"Connect to Triton\")\n",
        "    triton_client = tritonhttpclient.InferenceServerClient(_url)\n",
        "    \n",
        "    max_batch_size = 0\n",
        "    input_name = \"data_0\"\n",
        "    output_name = \"fc6_1\"\n",
        "    dtype = \"FP32\"\n",
        "\n",
        "@rawhttp\n",
        "def run(request):\n",
        "    if request.method == 'POST':\n",
        "        \n",
        "        reqBody = request.get_data(False)\n",
        "        img = Image.open(io.BytesIO(reqBody))\n",
        "        \n",
        "        image_data = preprocess(img, _scaling)\n",
        "        \n",
        "        input = tritonhttpclient.InferInput(input_name, image_data.shape, dtype)\n",
        "        input.set_data_from_numpy(image_data, binary_data=True)\n",
        "        output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=True, class_count=1)\n",
        "    \n",
        "        res = triton_client.infer(_model,\n",
        "                                [input],\n",
        "                                request_id=\"0\",\n",
        "                                outputs=[output])\n",
        "\n",
        "        result = postprocess(res, output_name, 1, max_batch_size > 0)\n",
        "\n",
        "        return AMLResponse(result, 200)\n",
        "    else:\n",
        "        return AMLResponse(\"bad request\", 500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "triton_client_url = 'https://github.com/NVIDIA/triton-inference-server/releases/download/v2.1.0/v2.1.0_ubuntu1804.clients.tar.gz'\n",
        "\n",
        "client_filename = 'tritonclientutils-2.1.0-py3-none-any.whl'\n",
        "clientutils_filename = 'tritonhttpclient-2.1.0-py3-none-any.whl'\n",
        "target_folder = 'python_client'\n",
        "\n",
        "if not os.path.exists(target_folder):\n",
        "    response = requests.get(triton_client_url)\n",
        "    archive = tarfile.open(fileobj=BytesIO(response.content))\n",
        "    with tempfile.TemporaryDirectory() as temp_folder:\n",
        "        archive.extractall(temp_folder)\n",
        "        os.mkdir(target_folder)\n",
        "        shutil.copy(os.path.join(temp_folder, 'python', client_filename), target_folder)\n",
        "        shutil.copy(os.path.join(temp_folder, 'python', clientutils_filename), target_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now create the deployment configuration objects and deploy the model as a webservice.\n",
        "\n",
        "** Note: ** If you use the Triton python client, also include them in the `Environment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Set the web service configuration (using default here)\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AksWebservice\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.environment import Environment\n",
        "\n",
        "env = Environment(\"deploytocloudenv\")\n",
        "conda_dep = CondaDependencies(\"env.yml\")\n",
        "client_whl_url = Environment.add_private_pip_wheel(workspace=ws, file_path = os.path.join(target_folder, client_filename), exist_ok=True)\n",
        "clientutils_whl_url = Environment.add_private_pip_wheel(workspace=ws, file_path = os.path.join(target_folder, clientutils_filename), exist_ok=True)\n",
        "conda_dep.add_pip_package(client_whl_url)\n",
        "conda_dep.add_pip_package(clientutils_whl_url)\n",
        "env.python.conda_dependencies = conda_dep\n",
        "\n",
        "# TODO: replace with MCR image name\n",
        "#env.docker.base_image = DEFAULT_GPU_IMAGE\n",
        "env.docker.base_image = \"delliwscentrec595b96.azurecr.io/azureml-tritonserver:20.07-py3\"\n",
        "env.docker.base_image_registry.address = \"delliwscentrec595b96.azurecr.io\"\n",
        "env.docker.base_image_registry.username = \"delliwscentrec595b96\"\n",
        "env.docker.base_image_registry.password = \"eOlKQlx+tFa+J3Ml5a86fOHf8i=hi/ec\"\n",
        "\n",
        "# Optionally specify a worker count to leverage the capability of concurrency and server-side batching from Triton\n",
        "# env.environment_variables = {\"WORKER_COUNT\":\"128\"}\n",
        "\n",
        "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)\n",
        "aks_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 4)\n",
        "\n",
        "# # Enable token auth and disable (key) auth on the webservice\n",
        "# aks_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 4, token_auth_enabled=True, auth_enabled=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "aks_service_name ='gpu-densenet-onnx'\n",
        "\n",
        "aks_service = Model.deploy(workspace=ws,\n",
        "                           name=aks_service_name,\n",
        "                           models=[model],\n",
        "                           inference_config=inference_config,\n",
        "                           deployment_config=aks_config,\n",
        "                           deployment_target=gpu_cluster)\n",
        "\n",
        "aks_service.wait_for_deployment(show_output = True)\n",
        "print(aks_service.state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "aks_service.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test the web service\n",
        "We test the web sevice by passing the test images content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import requests\n",
        "\n",
        "# if (key) auth is enabled, fetch keys and include in the request\n",
        "key1, key2 = aks_service.get_keys()\n",
        "\n",
        "headers = {'Content-Type':'application/octet-stream', 'Authorization': 'Bearer ' + key1}\n",
        "\n",
        "# # if token auth is enabled, fetch token and include in the request\n",
        "# access_token, fetch_after = aks_service.get_token()\n",
        "# headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + access_token}\n",
        "\n",
        "test_sample = open('car.jpg', 'rb').read()\n",
        "resp = requests.post(aks_service.scoring_uri, test_sample, headers=headers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Clean up\n",
        "Delete the service, image, model and compute target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "aks_service.delete()\n",
        "model.delete()\n",
        "gpu_cluster.delete()\n"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "vaidyas"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3.6.9 64-bit",
      "language": "python",
      "name": "python36964bit49f9d8f83b294f2eb4e5a3f7c26b67fb"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}