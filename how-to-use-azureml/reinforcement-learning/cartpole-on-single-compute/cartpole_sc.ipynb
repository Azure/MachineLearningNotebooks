{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/reinforcement-learning/cartpole_on_single_compute/cartpole_sc.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reinforcement Learning in Azure Machine Learning - Cartpole Problem on Single Compute\n",
        "\n",
        "Reinforcement Learning in Azure Machine Learning is a managed service for running reinforcement learning training and simulation. With Reinforcement Learning in Azure Machine Learning, data scientists can start developing reinforcement learning systems on one machine, and scale to compute targets with 100s of nodes if needed.\n",
        "\n",
        "This example shows how to use Reinforcement Learning in Azure Machine Learning to train a Cartpole playing agent on a single compute. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cartpole problem\n",
        "\n",
        "Cartpole, also known as [Inverted Pendulum](https://en.wikipedia.org/wiki/Inverted_pendulum), is a pendulum with a center of mass above its pivot point. This formation is essentially unstable and will easily fall over but can be kept balanced by applying appropriate horizontal forces to the pivot point.\n",
        "\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <th>\n",
        "      <img src=\"./images/cartpole.png\" alt=\"Cartpole image\" /> \n",
        "    </th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "      <th><p>Fig 1. Cartpole problem schematic description (from <a href=\"https://towardsdatascience.com/cartpole-introduction-to-reinforcement-learning-ed0eb5b58288\">towardsdatascience.com</a>).</p></th>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "The goal here is to train an agent to keep the cartpole balanced by applying appropriate forces to the pivot point.\n",
        "\n",
        "See [this video](https://www.youtube.com/watch?v=XiigTGKZfks) for a real-world demonstration of cartpole problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prerequisite\n",
        "The user should have completed the Azure Machine Learning Tutorial: [Get started creating your first ML experiment with the Python SDK](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-sdk-setup). You will need to make sure that you have a valid subscription ID, a resource group, and an Azure Machine Learning workspace. All datastores and datasets you use should be associated with your workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up Development Environment\n",
        "The following subsections show typical steps to setup your development environment. Setup includes:\n",
        "\n",
        "* Connecting to a workspace to enable communication between your local machine and remote resources\n",
        "* Creating an experiment to track all your runs\n",
        "* Creating a remote compute target to use for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Azure Machine Learning SDK \n",
        "Display the Azure Machine Learning SDK version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646347616697
        }
      },
      "outputs": [],
      "source": [
        "import azureml.core\n",
        "\n",
        "print(\"Azure Machine Learning SDK version:\", azureml.core.VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Azure Machine Learning workspace\n",
        "Get a reference to an existing Azure Machine Learning workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646429058500
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.location, ws.resource_group, sep = ' | ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a new compute resource or attach an existing one\n",
        "\n",
        "A compute target is a designated compute resource where you run your training and simulation scripts. This location may be your local machine or a cloud-based compute resource. The code below shows how to create a cloud-based compute target. For more information see [What are compute targets in Azure Machine Learning?](https://docs.microsoft.com/en-us/azure/machine-learning/concept-compute-target)\n",
        "\n",
        "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
        "\n",
        "**Note: Creation of a compute resource can take several minutes**. Please make sure to change `STANDARD_D2_V2` to a [size available in your region](https://azure.microsoft.com/en-us/global-infrastructure/services/?products=virtual-machines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646359152101
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.compute import AmlCompute, ComputeTarget\n",
        "import os\n",
        "\n",
        "# Choose a name and maximum size for your cluster\n",
        "compute_name = \"cpu-cluster-d2\"\n",
        "compute_min_nodes = 0\n",
        "compute_max_nodes = 4\n",
        "vm_size = \"STANDARD_D2_V2\"\n",
        "\n",
        "if compute_name in ws.compute_targets:\n",
        "    print(\"Found an existing compute target of name: \" + compute_name)\n",
        "    compute_target = ws.compute_targets[compute_name]\n",
        "    # Note: you may want to make sure compute_target is of type AmlCompute        \n",
        "else:\n",
        "    print(\"Creating new compute target...\")\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(\n",
        "        vm_size=vm_size,\n",
        "        min_nodes=compute_min_nodes, \n",
        "        max_nodes=compute_max_nodes)\n",
        "        \n",
        "    # Create the cluster\n",
        "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
        "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
        "\n",
        "print(compute_target.get_status().serialize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Azure Machine Learning experiment\n",
        "Create an experiment to track the runs in your workspace. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646348040613
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.experiment import Experiment\n",
        "\n",
        "experiment_name = 'CartPole-v0-SC'\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646417962898
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Environment\n",
        "import os\n",
        "\n",
        "ray_environment_name = 'cartpole-ray-sc'\n",
        "ray_environment_dockerfile_path = os.path.join(os.getcwd(), 'files', 'docker', 'Dockerfile')\n",
        "\n",
        "# Build environment image\n",
        "ray_environment = Environment. \\\n",
        "    from_dockerfile(name=ray_environment_name, dockerfile=ray_environment_dockerfile_path). \\\n",
        "    register(workspace=ws)\n",
        "ray_env_build_details = ray_environment.build(workspace=ws)\n",
        "\n",
        "ray_env_build_details.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Cartpole Agent\n",
        "In this section, we show how to use Azure Machine Learning jobs and Ray/RLlib framework to train a cartpole playing agent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create reinforcement learning training run\n",
        "\n",
        "The code below submits the training run using a `ScriptRunConfig`. By providing the\n",
        "command to run the training, and a `RunConfig` object configured with your\n",
        "compute target, number of nodes, and environment image to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646437786449
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import RunConfiguration, ScriptRunConfig, Experiment\n",
        "from azureml.core.runconfig import DockerConfiguration, RunConfiguration\n",
        "\n",
        "training_algorithm = \"PPO\"\n",
        "rl_environment = \"CartPole-v0\"\n",
        "video_capture = True\n",
        "if video_capture:\n",
        "    algorithm_config = '\\'{\"num_gpus\": 0, \"num_workers\": 1, \"monitor\": true}\\''\n",
        "else:\n",
        "    algorithm_config = '\\'{\"num_gpus\": 0, \"num_workers\": 1, \"monitor\": false}\\''\n",
        "\n",
        "script_name = 'cartpole_training.py'\n",
        "script_arguments = [\n",
        "    '--run', training_algorithm,\n",
        "    '--env', rl_environment,\n",
        "    '--stop', '\\'{\"episode_reward_mean\": 200, \"time_total_s\": 300}\\'',\n",
        "    '--config', algorithm_config,\n",
        "    '--checkpoint-freq', '2',\n",
        "    '--checkpoint-at-end',\n",
        "    '--local-dir', './logs'\n",
        "]\n",
        "\n",
        "ray_environment = Environment.get(ws, name=ray_environment_name)\n",
        "run_config = RunConfiguration(communicator='OpenMpi')\n",
        "run_config.target = compute_target\n",
        "run_config.docker = DockerConfiguration(use_docker=True)\n",
        "run_config.node_count = 1\n",
        "run_config.environment = ray_environment\n",
        "command=[\"python\", script_name, *script_arguments]\n",
        "\n",
        "if video_capture:\n",
        "    command = [\"xvfb-run -s '-screen 0 640x480x16 -ac +extension GLX +render' \"] + command\n",
        "    run_config.environment_variables[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "\n",
        "trainint_config = ScriptRunConfig(source_directory='./files',\n",
        "            command=command,\n",
        "            run_config = run_config\n",
        "            )\n",
        "\n",
        "training_run = experiment.submit(trainint_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training script\n",
        "\n",
        "As recommended in RLlib documentations, we use Ray Tune API to run the training algorithm. All the RLlib built-in trainers are compatible with the Tune API. Here we use `tune.run()` to execute a built-in training algorithm. For convenience, down below you can see part of the entry script where we make this call.\n",
        "\n",
        "This is the list of parameters we are passing into `tune.run()` via the `script_params` parameter:\n",
        "\n",
        "- `run_or_experiment`: name of the [built-in algorithm](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#rllib-algorithms), 'PPO' in our example,\n",
        "- `config`: Algorithm-specific configuration. This includes specifying the environment, `env`, which in our example is the gym **[CartPole-v0](https://gym.openai.com/envs/CartPole-v0/)** environment,\n",
        "- `stop`: stopping conditions, which could be any of the metrics returned by the trainer. Here we use \"mean of episode reward\", and \"total training time in seconds\" as stop conditions, and\n",
        "- `checkpoint_freq` and `checkpoint_at_end`: Frequency of taking checkpoints (number of training iterations between checkpoints), and if a checkpoint should be taken at the end.\n",
        "\n",
        "We also specify the `local_dir`, the directory in which the training logs, checkpoints and other training artificats will be recorded. \n",
        "\n",
        "See [RLlib Training APIs](https://ray.readthedocs.io/en/latest/rllib-training.html#rllib-training-apis) for more details, and also [Training (tune.run, tune.Experiment)](https://ray.readthedocs.io/en/latest/tune/api_docs/execution.html#training-tune-run-tune-experiment) for the complete list of parameters.\n",
        "\n",
        "```python\n",
        "import ray\n",
        "import ray.tune as tune\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # parse arguments ...\n",
        "    \n",
        "    # Start ray head (single node)\n",
        "    os.system('ray start --head')\n",
        "    ray.init(address='auto')\n",
        "\n",
        "    # Run training task using tune.run\n",
        "    tune.run(\n",
        "        run_or_experiment=args.run,\n",
        "        config=dict(args.config, env=args.env),\n",
        "        stop=args.stop,\n",
        "        checkpoint_freq=args.checkpoint_freq,\n",
        "        checkpoint_at_end=args.checkpoint_at_end,\n",
        "        local_dir=args.local_dir\n",
        "    )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monitor experiment\n",
        "\n",
        "Azure Machine Learning provides a Jupyter widget to show the status of an experiment run. You could use this widget to monitor the status of the runs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646437627002
        }
      },
      "outputs": [],
      "source": [
        "from azureml.widgets import RunDetails\n",
        "\n",
        "RunDetails(training_run).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stop the run\n",
        "To stop the run, call `training_run.cancel()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment line below to cancel the run\n",
        "# training_run.cancel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wait for completion\n",
        "Wait for the run to complete before proceeding.\n",
        "\n",
        "**Note: The length of the run depends on the provisioning time of the compute target and it may take several minutes to complete.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_run.wait_for_completion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get access to training artifacts\n",
        "We can simply use run id to get a handle to an in-progress or a previously concluded run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core import Run\n",
        "\n",
        "run_id = training_run.id # Or set to run id of a completed run (e.g. 'rl-cartpole-v0_1587572312_06e04ace_head')\n",
        "run = Run(experiment, run_id=run_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use the Run API to download policy training artifacts (saved model and checkpoints) to local compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646437652309
        }
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "from distutils import dir_util\n",
        "\n",
        "training_artifacts_path = path.join(\"logs\", training_algorithm)\n",
        "print(\"Training artifacts path:\", training_artifacts_path)\n",
        "\n",
        "if path.exists(training_artifacts_path):\n",
        "    dir_util.remove_tree(training_artifacts_path)\n",
        "\n",
        "# Download run artifacts to local compute\n",
        "training_run.download_files(training_artifacts_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display movies of selected training episodes\n",
        "\n",
        "Ray creates video output of selected training episodes in mp4 format.  Here we will display two of these, i.e. the first and the last recorded videos, so you could see the improvement of the agent after training.\n",
        "\n",
        "First we introduce a few helper functions: a function to download the movies from our dataset, another one to find mp4 movies in a local directory, and one more to display a downloaded movie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646437657045
        }
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# A helper function to find movies in a directory\n",
        "def find_movies(movie_path):\n",
        "    print(\"Looking in path:\", movie_path)\n",
        "    mp4_movies = []\n",
        "    for root, _, files in os.walk(movie_path):\n",
        "        for name in files:\n",
        "            if name.endswith('.mp4'):\n",
        "                mp4_movies.append(path.join(root, name))\n",
        "    print('Found {} movies'.format(len(mp4_movies)))\n",
        "\n",
        "    return mp4_movies\n",
        "\n",
        "\n",
        "# A helper function to display a movie\n",
        "from IPython.core.display import Video\n",
        "from IPython.display import display\n",
        "def display_movie(movie_file):\n",
        "    display(Video(movie_file, embed=True, html_attributes='controls'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look for the downloaded movies in the local directory and sort them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646437690241
        }
      },
      "outputs": [],
      "source": [
        "mp4_files = find_movies(training_artifacts_path)\n",
        "mp4_files.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display a movie of the first training episode.  This is how the agent performs with no training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646437692954
        }
      },
      "outputs": [],
      "source": [
        "first_movie = mp4_files[0] if len(mp4_files) > 0 else None\n",
        "print(\"First movie:\", first_movie)\n",
        "\n",
        "display_movie(first_movie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display a movie of the last training episode.  This is how a fully-trained agent performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1646437717147
        }
      },
      "outputs": [],
      "source": [
        "last_movie = mp4_files[-1] if len(mp4_files) > 0 else None\n",
        "print(\"Last movie:\", last_movie)\n",
        "\n",
        "display_movie(last_movie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Trained Agent and See Results\n",
        "\n",
        "We can evaluate a previously trained policy using the `rollout.py` helper script provided by RLlib (see [Evaluating Trained Policies](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) for more details). Here we use an adaptation of this script to reconstruct a policy from a checkpoint taken and saved during training. We took these checkpoints by setting `checkpoint-freq` and `checkpoint-at-end` parameters above.\n",
        "In this section we show how to use these checkpoints to evaluate the trained policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate a trained policy\n",
        "In this section, we submit another job, to evalute a trained policy. The entrypoint for this job is\n",
        "`cartpole-rollout.py` script, and we we pass the checkpoints dataset to this script as a dataset refrence.\n",
        "\n",
        "We are using script parameters to pass in the same algorithm and the same environment used during training. We also specify the checkpoint number of the checkpoint we wish to evaluate, `checkpoint-number`, and number of the steps we shall run the rollout, `steps`.\n",
        "\n",
        "The training artifacts dataset will be accessible to the rollout script as a mounted folder. The mounted folder and the checkpoint number, passed in via `checkpoint-number`, will be used to create a path to the checkpoint we are going to evaluate. The created checkpoint path then will be passed into RLlib rollout script for evaluation.\n",
        "\n",
        "Let's find the checkpoints and the last checkpoint number first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A helper function to find checkpoint files in a directory\n",
        "def find_checkpoints(file_path):\n",
        "    print(\"Looking in path:\", file_path)\n",
        "    checkpoints = []\n",
        "    for root, _, files in os.walk(file_path):\n",
        "        for name in files:\n",
        "            if os.path.basename(root).startswith('checkpoint_'):\n",
        "                checkpoints.append(path.join(root, name))\n",
        "    return checkpoints\n",
        "\n",
        "checkpoint_files = find_checkpoints(training_artifacts_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find checkpoints and last checkpoint number\n",
        "checkpoint_numbers = []\n",
        "for file in checkpoint_files:\n",
        "    file = os.path.basename(file)\n",
        "    if file.startswith('checkpoint-') and not file.endswith('.tune_metadata'):\n",
        "        checkpoint_numbers.append(int(file.split('-')[-1]))\n",
        "\n",
        "print(\"Checkpoints:\", checkpoint_numbers)\n",
        "\n",
        "last_checkpoint_number = max(checkpoint_numbers)\n",
        "print(\"Last checkpoint number:\", last_checkpoint_number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the checkpoint files and create a DataSet\n",
        "from azureml.core import Dataset\n",
        "\n",
        "datastore = ws.get_default_datastore()\n",
        "checkpoint_dataref = datastore.upload_files(checkpoint_files, target_path='cartpole_checkpoints_' + run_id, overwrite=True)\n",
        "checkpoint_ds = Dataset.File.from_files(checkpoint_dataref)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can submit the training run using a `ScriptRunConfig`. By providing the\n",
        "command to run the training, and a `RunConfig` object configured w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ray_environment_name = 'cartpole-ray-sc'\n",
        "\n",
        "experiment_name = 'CartPole-v0-SC'\n",
        "training_algorithm = 'PPO'\n",
        "rl_environment = 'CartPole-v0'\n",
        "\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)\n",
        "ray_environment = Environment.get(workspace=ws, name=ray_environment_name)\n",
        "\n",
        "script_name = 'cartpole_rollout.py'\n",
        "video_capture = True\n",
        "if video_capture:\n",
        "    script_arguments = ['--video-dir', './logs/video']\n",
        "else:\n",
        "    script_arguments = ['--no-render']\n",
        "script_arguments = script_arguments + [\n",
        "    '--run', training_algorithm,\n",
        "    '--env', rl_environment,\n",
        "    '--config', '{}',\n",
        "    '--steps', '2000',\n",
        "    '--checkpoint-number', str(last_checkpoint_number),\n",
        "    '--artifacts-dataset', checkpoint_ds.as_named_input('artifacts_dataset'),\n",
        "    '--artifacts-path', checkpoint_ds.as_named_input('artifacts_path').as_mount()\n",
        "]\n",
        "\n",
        "command = [\"python\", script_name, *script_arguments]\n",
        "\n",
        "if video_capture:\n",
        "    command = [\"xvfb-run -s '-screen 0 640x480x16 -ac +extension GLX +render' \"] + command\n",
        "    run_config.environment_variables[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "\n",
        "run_config = RunConfiguration(communicator='OpenMpi')\n",
        "run_config.target = compute_target\n",
        "run_config.docker = DockerConfiguration(use_docker=True)\n",
        "run_config.node_count = 1\n",
        "run_config.environment = ray_environment\n",
        "\n",
        "rollout_config = ScriptRunConfig(\n",
        "                    source_directory='./files',\n",
        "                    command=command,\n",
        "                    run_config=run_config\n",
        "                   )\n",
        "\n",
        "rollout_run = experiment.submit(rollout_config)\n",
        "rollout_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then, similar to the training section, we can monitor the real-time progress of the rollout run and its chid as follows. If you browse logs of the child run you can see the evaluation results recorded in driver_log.txt file. Note that you may need to wait several minutes before these results become available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RunDetails(rollout_run).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Wait for completion of the rollout run before moving to the next section, or you may cancel the run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment line below to cancel the run\n",
        "#rollout_run.cancel()\n",
        "rollout_run.wait_for_completion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Display movies of selected rollout episodes\n",
        "\n",
        "To display recorded movies first we download recorded videos to local machine. Here again we create a dataset of rollout artifacts and use the helper functions introduced above to download and displays rollout videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download rollout artifacts\n",
        "rollout_artifacts_path = path.join(\"logs\", \"rollout\")\n",
        "print(\"Rollout artifacts path:\", rollout_artifacts_path)\n",
        "\n",
        "if path.exists(rollout_artifacts_path):\n",
        "    dir_util.remove_tree(rollout_artifacts_path)\n",
        "\n",
        "# Download videos to local compute\n",
        "rollout_run.download_files(\"logs/video\", output_directory = rollout_artifacts_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, similar to the training section, we look for the last video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look for the downloaded movie in local directory\n",
        "mp4_files = find_movies(rollout_artifacts_path)\n",
        "mp4_files.sort()\n",
        "last_movie = mp4_files[-1] if len(mp4_files) > 1 else None\n",
        "print(\"Last movie:\", last_movie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display last video recorded during the rollout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "last_movie = mp4_files[-1] if len(mp4_files) > 0 else None\n",
        "print(\"Last movie:\", last_movie)\n",
        "\n",
        "display_movie(last_movie)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning up\n",
        "For your convenience, below you can find code snippets to clean up any resources created as part of this tutorial that you don't wish to retain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To archive the created experiment:\n",
        "#exp.archive()\n",
        "\n",
        "# To delete the compute target:\n",
        "#compute_target.delete()\n",
        "\n",
        "# To delete downloaded training artifacts\n",
        "#if os.path.exists(training_artifacts_path):\n",
        "#    dir_util.remove_tree(training_artifacts_path)\n",
        "\n",
        "# To delete downloaded rollout videos\n",
        "#if path.exists(rollout_artifacts_path):\n",
        "#    dir_util.remove_tree(rollout_artifacts_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next\n",
        "This example was about running Reinforcement Learning in Azure Machine Learning (Ray/RLlib Framework) on a single compute. Please see [Pong Problem](../atari-on-distributed-compute/pong_rllib.ipynb)\n",
        "example which uses Ray RLlib to train a Pong playing agent on a multi-node cluster."
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "hoazari"
      },
      {
        "name": "dasommer"
      }
    ],
    "categories": [
      "how-to-use-azureml",
      "reinforcement-learning"
    ],
    "interpreter": {
      "hash": "13382f70c1d0595120591d2e358c8d446daf961bf951d1fba9a32631e205d5ab"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.6",
      "language": "python",
      "name": "python36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "notice": "Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT License.",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}